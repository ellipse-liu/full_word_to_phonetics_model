{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f6889be-fe9e-4cee-a326-8a61d5c52de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Concatenate, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from BahdanauAttention import AttentionLayer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import random\n",
    "import eng_to_ipa as ipa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6df789-39ce-42f5-bdbf-7f78dd349cb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bi_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, num_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 256\n",
    "        self.embedding_dim = 200\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "\n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs) \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len,)) \n",
    "        # Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000\n",
    "        self.enc_emb = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable = True)(self.encoder_inputs)\n",
    "\n",
    "        # Bidirectional lstm layer\n",
    "        self.enc_lstm1 = Bidirectional(LSTM(self.latent_dim,return_sequences=True,return_state=True))\n",
    "        self.encoder_outputs1, self.forw_state_h, self.forw_state_c, self.back_state_h, self.back_state_c = self.enc_lstm1(self.enc_emb)\n",
    "\n",
    "        # Concatenate both h and c \n",
    "        self.final_enc_h = Concatenate()([self.forw_state_h,self.back_state_h])\n",
    "        self.final_enc_c = Concatenate()([self.forw_state_c,self.back_state_c])\n",
    "\n",
    "        # get Context vector\n",
    "        self.encoder_states =[self.final_enc_h, self.final_enc_c]\n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,)) \n",
    "\n",
    "        # decoder embedding with same number as encoder embedding\n",
    "        self.dec_emb_layer = Embedding(self.num_decoder_vocab, self.embedding_dim) \n",
    "        self.dec_emb = self.dec_emb_layer(self.decoder_inputs)   # apply this way because we need embedding layer for prediction \n",
    "\n",
    "        # In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
    "        # LSTM using encoder's final states as initial state\n",
    "        self.decoder_lstm = LSTM(self.latent_dim*2, return_sequences=True, return_state=True) \n",
    "        self.decoder_outputs, _, _ = self.decoder_lstm(self.dec_emb, initial_state=self.encoder_states)\n",
    "\n",
    "        # Using Attention Layer\n",
    "        self.attention_layer = AttentionLayer()\n",
    "        self.attention_result, self.attention_weights = self.attention_layer([self.encoder_outputs1, self.decoder_outputs])\n",
    "\n",
    "        # Concat attention output and decoder LSTM output \n",
    "        self.decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs, self.attention_result])\n",
    "\n",
    "        # Dense layer with softmax\n",
    "        self.decoder_dense = Dense(self.num_decoder_vocab, activation='softmax')\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_concat_input)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "    \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='segmenter_best_weights.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "\n",
    "    def build_inference_model(self):\n",
    "        self.encoder_model_inference = Model(self.encoder_inputs, outputs = [self.encoder_outputs1, self.final_enc_h, self.final_enc_c])\n",
    "        self.encoder_model_inference.save('final_encoder_model_segmenter.h5')\n",
    "\n",
    "        # Decoder Inference\n",
    "        self.decoder_state_h = Input(shape=(self.latent_dim*2,)) # This numbers has to be same as units of lstm's on which model is trained\n",
    "        self.decoder_state_c = Input(shape=(self.latent_dim*2,))\n",
    "\n",
    "        # we need hidden state for attention layer\n",
    "        # 36 is maximum length if english sentence It has to same as input taken by attention layer can see in model plot\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len,self.latent_dim*2)) \n",
    "        # get decoder states\n",
    "        self.dec_states = [self.decoder_state_h, self.decoder_state_c]\n",
    "\n",
    "        # embedding layer \n",
    "        self.dec_emb2 = self.dec_emb_layer(self.decoder_inputs)\n",
    "        self.decoder_outputs2, self.state_h2, self.state_c2 = self.decoder_lstm(self.dec_emb2, initial_state=self.dec_states)\n",
    "\n",
    "        # Attention inference\n",
    "        self.attention_result_inf, self.attention_weights_inf = self.attention_layer([self.decoder_hidden_state_input, self.decoder_outputs2])\n",
    "        self.decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs2, self.attention_result_inf])\n",
    "\n",
    "        self.dec_states2= [self.state_h2, self.state_c2]\n",
    "        self.decoder_outputs2 = self.decoder_dense(self.decoder_concat_input_inf)\n",
    "\n",
    "        # get decoder model\n",
    "        self.decoder_model_inference= Model(\n",
    "                            [self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_h, self.decoder_state_c],\n",
    "                             [self.decoder_outputs2]+ self.dec_states2)\n",
    "        self.decoder_model_inference.save('final_decoder_model_segmenter.h5')\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out ,e_h, e_c = self.encoder_model_inference.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.decoder_model_inference.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39b6787b-375c-4c55-8d4e-c9cc98c4a229",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch metadata word2seq\n",
    "metadata_file = open('metadata.txt')\n",
    "metadata = metadata_file.readlines()\n",
    "metadata = [line.strip('\\n') for line in metadata]\n",
    "metadata = [line.split('\\t') for line in metadata]\n",
    "metadata = [line[-1] for line in metadata]\n",
    "\n",
    "max_encoder_len_w2s = int(metadata[0])\n",
    "max_decoder_len_w2s = int(metadata[1])\n",
    "num_encoder_vocab_w2s = int(metadata[2])\n",
    "num_decoder_vocab_w2s = int(metadata[3])\n",
    "\n",
    "# fetch dictionaries\n",
    "e2i_file = open('e2i.pkl', 'rb')\n",
    "i2e_file = open('i2e.pkl', 'rb')\n",
    "d2i_file = open('d2i.pkl', 'rb')\n",
    "i2d_file = open('i2d.pkl', 'rb')\n",
    "\n",
    "e2i_w2s = pickle.load(e2i_file)\n",
    "i2e_w2s = pickle.load(i2e_file)\n",
    "d2i_w2s = pickle.load(d2i_file)\n",
    "i2d_w2s = pickle.load(i2d_file)\n",
    "\n",
    "e2i_file.close()\n",
    "i2e_file.close()\n",
    "d2i_file.close()\n",
    "i2d_file.close()\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875de411-9e73-4901-8c93-f8236c4d0f0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch metadata syl2phone\n",
    "metadata_file = open('metadata_ipa.txt')\n",
    "metadata = metadata_file.readlines()\n",
    "metadata = [line.strip('\\n') for line in metadata]\n",
    "metadata = [line.split('\\t') for line in metadata]\n",
    "metadata = [line[-1] for line in metadata]\n",
    "\n",
    "max_encoder_len_s2i = int(metadata[0])\n",
    "max_decoder_len_s2i = int(metadata[1])\n",
    "num_encoder_vocab_s2i = int(metadata[2])\n",
    "num_decoder_vocab_s2i = int(metadata[3])\n",
    "\n",
    "# fetch dictionaries\n",
    "e2i_file = open('ipa_e2i.pkl', 'rb')\n",
    "i2e_file = open('ipa_i2e.pkl', 'rb')\n",
    "d2i_file = open('ipa_d2i.pkl', 'rb')\n",
    "i2d_file = open('ipa_i2d.pkl', 'rb')\n",
    "\n",
    "e2i_s2i = pickle.load(e2i_file)\n",
    "i2e_s2i= pickle.load(i2e_file)\n",
    "d2i_s2i = pickle.load(d2i_file)\n",
    "i2d_s2i = pickle.load(i2d_file)\n",
    "\n",
    "e2i_file.close()\n",
    "i2e_file.close()\n",
    "d2i_file.close()\n",
    "i2d_file.close()\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd253bb8-4fbc-40ed-9db1-165f4efa974a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch metadata ipa_corrector\n",
    "metadata_file = open('metadata_ipa2ipa.txt')\n",
    "metadata = metadata_file.readlines()\n",
    "metadata = [line.strip('\\n') for line in metadata]\n",
    "metadata = [line.split('\\t') for line in metadata]\n",
    "metadata = [line[-1] for line in metadata]\n",
    "\n",
    "max_encoder_len_i2i = int(metadata[0])\n",
    "max_decoder_len_i2i = int(metadata[1])\n",
    "num_encoder_vocab_i2i = int(metadata[2])\n",
    "num_decoder_vocab_i2i = int(metadata[3])\n",
    "\n",
    "# fetch dictionaries\n",
    "e2i_file = open('ipa2ipa_e2i.pkl', 'rb')\n",
    "i2e_file = open('ipa2ipa_i2e.pkl', 'rb')\n",
    "d2i_file = open('ipa2ipa_d2i.pkl', 'rb')\n",
    "i2d_file = open('ipa2ipa_i2d.pkl', 'rb')\n",
    "\n",
    "e2i_i2i = pickle.load(e2i_file)\n",
    "i2e_i2i = pickle.load(i2e_file)\n",
    "d2i_i2i = pickle.load(d2i_file)\n",
    "i2d_i2i = pickle.load(i2d_file)\n",
    "\n",
    "e2i_file.close()\n",
    "i2e_file.close()\n",
    "d2i_file.close()\n",
    "i2d_file.close()\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fefb670c-1c63-4ed9-89a7-b73932c9e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch evaluation data\n",
    "training_data_file = open('training_data.txt')\n",
    "raw_data = training_data_file.readlines()\n",
    "random.shuffle(raw_data)\n",
    "raw_data = [line.strip('\\n') for line in raw_data]\n",
    "raw_split_data = [line.split('\\t') for line in raw_data]\n",
    "raw_english_words = [line[0] for line in raw_split_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1bfbe3-6890-4309-9657-c6ddf5fb46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = bi_model(max_encoder_len_w2s, max_decoder_len_w2s, num_encoder_vocab_w2s, num_decoder_vocab_w2s)\n",
    "syl2ipa = bi_model(max_encoder_len_s2i, max_decoder_len_s2i, num_encoder_vocab_s2i, num_decoder_vocab_s2i)\n",
    "ipa_corrector = bi_model(max_encoder_len_i2i, max_decoder_len_i2i, num_encoder_vocab_i2i, num_decoder_vocab_i2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad97f08-4ae3-458d-af44-5c10f05ceb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "segmenter.training_model.load_weights('segmenter_best_weights.h5')\n",
    "syl2ipa.training_model.load_weights('syllable_translator_best_weights.h5')\n",
    "ipa_corrector.training_model.load_weights('ipa_corrector_best_weights.h5')\n",
    "\n",
    "segmenter.build_inference_model()\n",
    "syl2ipa.build_inference_model()\n",
    "ipa_corrector.build_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "460b94ae-0828-4a58-add0-3302f720b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type a word:  crazily\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cra;zi;ly\n",
      "krɑzilaɪ\n",
      "ˈkrɑviəl\n"
     ]
    }
   ],
   "source": [
    "word = input(\"type a word: \")\n",
    "segmented_string, attempted_syllables, final_phonetics = full_stack_translate(word)\n",
    "\n",
    "print(segmented_string)\n",
    "print(attempted_syllables)\n",
    "print(final_phonetics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d6376-b41d-4949-a15b-0174a906073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def in_data(word, data):\n",
    "#     if [word] in eng_words_list[0]:\n",
    "#         return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0d3124a-3b8e-459a-a890-dd26773bf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2word(array):\n",
    "    final_string = \"\"\n",
    "    for c in array:\n",
    "        final_string += c\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b6bae7e-53a6-4fe0-931b-eb5b87dd0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_stack_translate(orig_word):\n",
    "    segmented_string = seq2word(segmenter.translate(orig_word, e2i_w2s, i2d_w2s, d2i_w2s))\n",
    "    segmented_array = segmented_string.split(';')\n",
    "    attempted_syllables = \"\"\n",
    "    for syl in segmented_array:\n",
    "        attempted_syllables += seq2word(syl2ipa.translate(syl, e2i_s2i, i2d_s2i, d2i_s2i))\n",
    "    final_phonetics = seq2word(ipa_corrector.translate(attempted_syllables, e2i_i2i, i2d_i2i, d2i_i2i))\n",
    "    return segmented_string, attempted_syllables, final_phonetics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c2da7c-92b3-4da1-8086-aa57cce402f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(array):\n",
    "    output_bleu = open('evaluation_metrics', 'w+', encoding = 'utf-8')\n",
    "    counter = 0\n",
    "    num_words = 0\n",
    "    sum_bleu_s2i = 0\n",
    "    sum_bleu_i2i = 0\n",
    "    for word in array:\n",
    "        counter += 1\n",
    "        print(counter, end='\\r')\n",
    "        actual_translation = ipa.convert(word)\n",
    "        if(not actual_translation.endswith('*')):\n",
    "            num_words += 1\n",
    "            segmented_string, attempted_syllables, final_phonetics = full_stack_translate(word)\n",
    "            bleu_score_s2i = sentence_bleu([*actual_translation], [*attempted_syllables], weights = [1])\n",
    "            bleu_score_i2i = sentence_bleu([*actual_translation], [*final_phonetics], weights = [1])\n",
    "            \n",
    "            sum_bleu_i2i += bleu_score_i2i\n",
    "            sum_bleu_s2i += bleu_score_s2i\n",
    "            \n",
    "            output_bleu.write(word + '\\t' + segmented_string + '\\t' + attempted_syllables + '\\t' + final_phonetics + actual_translation + '\\tAttempted_bleu:%.3f'%bleu_score_s2i +' \\tPost_adjust_bleu:%.3f'%bleu_score_i2i + '\\n')\n",
    "    output_bleu.write('Final metrics:\\n')\n",
    "    output_bleu.write('Word count: %i'%num_words + '\\n')\n",
    "    output_bleu.write('Average syllable to ipa bleu score: %.3f'%(sum_bleu_s2i/num_words) + '\\n')\n",
    "    output_bleu.write('Average post ipa adjustment bleu score: %.3f'%(sum_bleu_i2i/num_words))\n",
    "    output_bleu.close()\n",
    "    print(\"Completed!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2054e668-547d-4320-8fb2-a77623482320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "evaluation_words = raw_english_words[:1000]\n",
    "evaluate_bleu(evaluation_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05528b9-5cf0-4cfd-9b7b-b38c0a9c54e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
