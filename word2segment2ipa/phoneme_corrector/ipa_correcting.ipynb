{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec80e6d1-1f6f-4330-ba96-0b2c81805088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Concatenate, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from BahdanauAttention import AttentionLayer\n",
    "import numpy as np\n",
    "import random\n",
    "import eng_to_ipa as ipa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5590ed65-ef47-4cad-a8dd-cf3a72744d49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class bi_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, build_inference_model_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 256\n",
    "        self.embedding_dim = 200\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "\n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs) \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len,)) \n",
    "        # Embedding layer- i am using 1024 output-dim for embedding you can try diff values 100,256,512,1000\n",
    "        self.enc_emb = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable = True)(self.encoder_inputs)\n",
    "\n",
    "        # Bidirectional lstm layer\n",
    "        self.enc_lstm1 = Bidirectional(LSTM(self.latent_dim,return_sequences=True,return_state=True))\n",
    "        self.encoder_outputs1, self.forw_state_h, self.forw_state_c, self.back_state_h, self.back_state_c = self.enc_lstm1(self.enc_emb)\n",
    "\n",
    "        # Concatenate both h and c \n",
    "        self.final_enc_h = Concatenate()([self.forw_state_h,self.back_state_h])\n",
    "        self.final_enc_c = Concatenate()([self.forw_state_c,self.back_state_c])\n",
    "\n",
    "        # get Context vector\n",
    "        self.encoder_states =[self.final_enc_h, self.final_enc_c]\n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,)) \n",
    "\n",
    "        # decoder embedding with same number as encoder embedding\n",
    "        self.dec_emb_layer = Embedding(self.num_decoder_vocab, self.embedding_dim) \n",
    "        self.dec_emb = self.dec_emb_layer(self.decoder_inputs)   # apply this way because we need embedding layer for prediction \n",
    "\n",
    "        # In encoder we used Bidirectional so it's having two LSTM's so we have to take double units(256*2=512) for single decoder lstm\n",
    "        # LSTM using encoder's final states as initial state\n",
    "        self.decoder_lstm = LSTM(self.latent_dim*2, return_sequences=True, return_state=True) \n",
    "        self.decoder_outputs, _, _ = self.decoder_lstm(self.dec_emb, initial_state=self.encoder_states)\n",
    "\n",
    "        # Using Attention Layer\n",
    "        self.attention_layer = AttentionLayer()\n",
    "        self.attention_result, self.attention_weights = self.attention_layer([self.encoder_outputs1, self.decoder_outputs])\n",
    "\n",
    "        # Concat attention output and decoder LSTM output \n",
    "        self.decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs, self.attention_result])\n",
    "\n",
    "        # Dense layer with softmax\n",
    "        self.decoder_dense = Dense(self.num_decoder_vocab, activation='softmax')\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_concat_input)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "    \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='ipa_corrector_best_weights.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "\n",
    "    def build_inference_model(self):\n",
    "        self.encoder_model_inference = Model(self.encoder_inputs, outputs = [self.encoder_outputs1, self.final_enc_h, self.final_enc_c])\n",
    "        self.encoder_model_inference.save('final_encoder_model_ipa_corrector.h5')\n",
    "\n",
    "        # Decoder Inference\n",
    "        self.decoder_state_h = Input(shape=(self.latent_dim*2,)) # This numbers has to be same as units of lstm's on which model is trained\n",
    "        self.decoder_state_c = Input(shape=(self.latent_dim*2,))\n",
    "\n",
    "        # we need hidden state for attention layer\n",
    "        # 36 is maximum length if english sentence It has to same as input taken by attention layer can see in model plot\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len,self.latent_dim*2)) \n",
    "        # get decoder states\n",
    "        self.dec_states = [self.decoder_state_h, self.decoder_state_c]\n",
    "\n",
    "        # embedding layer \n",
    "        self.dec_emb2 = self.dec_emb_layer(self.decoder_inputs)\n",
    "        self.decoder_outputs2, self.state_h2, self.state_c2 = self.decoder_lstm(self.dec_emb2, initial_state=self.dec_states)\n",
    "\n",
    "        # Attention inference\n",
    "        self.attention_result_inf, self.attention_weights_inf = self.attention_layer([self.decoder_hidden_state_input, self.decoder_outputs2])\n",
    "        self.decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([self.decoder_outputs2, self.attention_result_inf])\n",
    "\n",
    "        self.dec_states2= [self.state_h2, self.state_c2]\n",
    "        self.decoder_outputs2 = self.decoder_dense(self.decoder_concat_input_inf)\n",
    "\n",
    "        # get decoder model\n",
    "        self.decoder_model_inference= Model(\n",
    "                            [self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_h, self.decoder_state_c],\n",
    "                             [self.decoder_outputs2]+ self.dec_states2)\n",
    "        self.decoder_model_inference.save('final_decoder_model_ipa_corrector.h5')\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out ,e_h, e_c = self.encoder_model_inference.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.decoder_model_inference.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc042a7e-890d-4ca5-87a5-9d6ba8e6b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters\n",
    "\n",
    "# fetch metadata\n",
    "metadata_file = open('metadata_ipa2ipa.txt')\n",
    "metadata = metadata_file.readlines()\n",
    "metadata = [line.strip('\\n') for line in metadata]\n",
    "metadata = [line.split('\\t') for line in metadata]\n",
    "metadata = [line[-1] for line in metadata]\n",
    "\n",
    "max_encoder_len = int(metadata[0])\n",
    "max_decoder_len = int(metadata[1])\n",
    "num_encoder_vocab = int(metadata[2])\n",
    "num_decoder_vocab = int(metadata[3])\n",
    "\n",
    "# fetch dictionaries\n",
    "e2i_file = open('ipa2ipa_e2i.pkl', 'rb')\n",
    "i2e_file = open('ipa2ipa_i2e.pkl', 'rb')\n",
    "d2i_file = open('ipa2ipa_d2i.pkl', 'rb')\n",
    "i2d_file = open('ipa2ipa_i2d.pkl', 'rb')\n",
    "\n",
    "e2i = pickle.load(e2i_file)\n",
    "i2e = pickle.load(i2e_file)\n",
    "d2i = pickle.load(d2i_file)\n",
    "i2d = pickle.load(i2d_file)\n",
    "\n",
    "e2i_file.close()\n",
    "i2e_file.close()\n",
    "d2i_file.close()\n",
    "i2d_file.close()\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621c83b9-c16b-41a0-b13a-3be9b98302bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch training data\n",
    "training_data_file = open('training_data_ipa2ipa.txt', encoding = 'UTF-8')\n",
    "raw_data = training_data_file.readlines()\n",
    "raw_data = [line.strip('\\n') for line in raw_data]\n",
    "raw_split_data = [line.split('\\t')[1:] for line in raw_data]\n",
    "attempted_list = [pair[0] for pair in raw_split_data]\n",
    "actual_list = [pair[1] for pair in raw_split_data]\n",
    "\n",
    "attempted_list = np.array(attempted_list).reshape((1,len(attempted_list)))\n",
    "actual_list = np.array(actual_list).reshape((1,len(actual_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077acf1-767c-4b81-aa7e-39a1943ec03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train data\n",
    "x_tr = []\n",
    "\n",
    "for syl in attempted_list[0]:\n",
    "    int_seq = []\n",
    "    for c in syl:\n",
    "        int_seq += [e2i[c]]\n",
    "    x_tr += [int_seq]\n",
    "    \n",
    "x_tr = pad_sequences(x_tr, maxlen=max_encoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbf0c81-7908-4661-ab48-b5175c3f34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y_train data\n",
    "y_tr = []\n",
    "\n",
    "for seq in actual_list[0]:\n",
    "    int_seq = []\n",
    "    for c in seq:\n",
    "        int_seq += [d2i[c]]\n",
    "    y_tr += [int_seq]\n",
    "y_tr = pad_sequences(y_tr, maxlen=max_decoder_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f39845-cddd-4f0f-95a7-c0170d33847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into validation and training\n",
    "\n",
    "split_index = int(len(x_tr) * .8)\n",
    "\n",
    "y_test = y_tr[split_index:]\n",
    "y_test_in = y_test[:, :-1]\n",
    "y_test_out = y_test[:, 1:]\n",
    "\n",
    "y_tr = y_tr[:split_index]\n",
    "y_tr_in = y_tr[:, :-1]\n",
    "y_tr_out = y_tr[:, 1:]\n",
    "\n",
    "x_test = x_tr[split_index:]\n",
    "x_tr = x_tr[:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9e72e3-c272-40e4-ae7e-a788a3556218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n"
     ]
    }
   ],
   "source": [
    "print(len(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53ec53b-f0b7-4f31-b81d-92ac1838211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa_corrector = bi_model(max_encoder_len, max_decoder_len, num_encoder_vocab, num_decoder_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83df1fe-2f59-4026-a60c-9d566735f192",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 31s 2s/step - loss: 2.3539 - acc: 0.5245 - val_loss: 1.5444 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61453, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.5144 - acc: 0.6147 - val_loss: 1.4299 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61453 to 0.62985, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.4382 - acc: 0.6297 - val_loss: 1.3571 - val_acc: 0.6320\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62985 to 0.63202, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 1.3632 - acc: 0.6404 - val_loss: 1.2615 - val_acc: 0.6816\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.63202 to 0.68156, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 1.3267 - acc: 0.6657 - val_loss: 1.2565 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.68156\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.2612 - acc: 0.6740 - val_loss: 1.2319 - val_acc: 0.6772\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.68156\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.2224 - acc: 0.6797 - val_loss: 1.2981 - val_acc: 0.6553\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.68156\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 1.2022 - acc: 0.6862 - val_loss: 1.1454 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.68156 to 0.69025, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 1.1465 - acc: 0.6977 - val_loss: 1.1128 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.69025 to 0.69957, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.0850 - acc: 0.7100 - val_loss: 1.0402 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.69957 to 0.72357, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.0251 - acc: 0.7219 - val_loss: 0.9862 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.72357 to 0.72855, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.9592 - acc: 0.7365 - val_loss: 0.9802 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72855\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.9228 - acc: 0.7432 - val_loss: 0.9556 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.72855 to 0.73468, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.8884 - acc: 0.7504 - val_loss: 0.8630 - val_acc: 0.7543\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.73468 to 0.75434, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.8264 - acc: 0.7653 - val_loss: 0.8569 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.75434 to 0.75613, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.7970 - acc: 0.7724 - val_loss: 0.8443 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.75613 to 0.76328, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 31s 3s/step - loss: 0.7708 - acc: 0.7787 - val_loss: 0.8007 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.76328 to 0.76749, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 26s 2s/step - loss: 0.7329 - acc: 0.7868 - val_loss: 0.8143 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76749\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.6976 - acc: 0.7980 - val_loss: 0.7058 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.76749 to 0.79201, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.6572 - acc: 0.8060 - val_loss: 0.7030 - val_acc: 0.7963\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.79201 to 0.79635, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.6242 - acc: 0.8165 - val_loss: 0.6907 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79635\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.5830 - acc: 0.8278 - val_loss: 0.6965 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79635\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.5662 - acc: 0.8318 - val_loss: 0.6077 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.79635 to 0.82457, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.5316 - acc: 0.8430 - val_loss: 0.6184 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.82457\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.4859 - acc: 0.8560 - val_loss: 0.6042 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.82457\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.4828 - acc: 0.8542 - val_loss: 0.6115 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.82457\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.4409 - acc: 0.8702 - val_loss: 0.5821 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.82457 to 0.82469, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.4222 - acc: 0.8737 - val_loss: 0.5372 - val_acc: 0.8415\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.82469 to 0.84155, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.3800 - acc: 0.8893 - val_loss: 0.5513 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84155\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.3796 - acc: 0.8854 - val_loss: 0.5083 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.84155 to 0.84921, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.3410 - acc: 0.8991 - val_loss: 0.5558 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84921\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.3130 - acc: 0.9091 - val_loss: 0.5227 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84921\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "ipa_corrector.compile()\n",
    "ipa_corrector.fit(x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233b7940-65ef-4667-844a-9961aa141af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.3424 - acc: 0.8980 - val_loss: 0.5014 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84921, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.3214 - acc: 0.9036 - val_loss: 0.5168 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.84921\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.3081 - acc: 0.9095 - val_loss: 0.4891 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84921 to 0.85304, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.2715 - acc: 0.9214 - val_loss: 0.4689 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85304 to 0.85674, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.2473 - acc: 0.9285 - val_loss: 0.4819 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85674 to 0.85930, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.2565 - acc: 0.9235 - val_loss: 0.4496 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85930 to 0.86364, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.2074 - acc: 0.9432 - val_loss: 0.4981 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86364\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.1984 - acc: 0.9451 - val_loss: 0.4421 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86364 to 0.87091, saving model to ipa_corrector_best_weights.h5\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.1973 - acc: 0.9432 - val_loss: 0.4416 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87091\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.1667 - acc: 0.9553 - val_loss: 0.4553 - val_acc: 0.8658\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87091\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 24s 2s/step - loss: 0.1646 - acc: 0.9551 - val_loss: 0.4540 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87091\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "# resume training\n",
    "ipa_corrector.training_model.load_weights('ipa_corrector_best_weights.h5')\n",
    "ipa_corrector.fit(x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0daa8e34-7185-4b8a-9dd0-4eadc4325992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "ipa_corrector.training_model.load_weights('ipa_corrector_best_weights.h5')\n",
    "ipa_corrector.build_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1503f0b-fcb3-485c-9891-83bed9d93a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prikɑraɪaʊz\n",
      "<priˈkɛriəs>\n",
      "priˈkɔriəs\n"
     ]
    }
   ],
   "source": [
    "word = attempted_list[0][987]\n",
    "print(seq2word(word))\n",
    "print(seq2word(actual_list[0][987]))\n",
    "\n",
    "print(seq2word(ipa_corrector.translate(word, e2i, i2d, d2i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc65588c-e673-4b4f-938c-364d99362579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2word(array):\n",
    "    final_string = \"\"\n",
    "    for c in array:\n",
    "        final_string += c\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae16f77-3b07-41c0-a60e-dc994c49c0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
